{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OwpjIKO9qiv"
      },
      "source": [
        "Creating tensors, mathematical ops, reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9sDRuSBqUXM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Initialization of Tensors\n",
        "x = tf.constant(4, shape=(1,1), dtype=tf.float32)\n",
        "x = tf.constant([[1,2,3], [4,5,6]])\n",
        "x = tf.ones((2,3))\n",
        "x = tf.zeros((3,3))\n",
        "x = tf.eye(3, dtype=tf.int32) # identity matrix 3x3\n",
        "x = tf.random.normal((3,3,4), mean=0, stddev=1) # a distribution\n",
        "x = tf.random. uniform((1,3), minval=0, maxval=1) # a uniform distribution\n",
        "x = tf.range(9)\n",
        "x = tf.range(start=1, limit=10, delta=2) # similar to python range\n",
        "x = tf.cast(x, dtype=tf.float64) # tf.float(16,32,64), tf.int(8,16,32,64), tf.bool\n",
        "\n",
        "#print(type(x))\n",
        "#print(x)\n",
        "\n",
        "# Mathematical Operations\n",
        "x = tf.constant([1,2,3])\n",
        "y = tf.constant([9,8,7])\n",
        "\n",
        "z = tf.add(x, y) # an element wise addition\n",
        "z = x + y # overloaded addition\n",
        "\n",
        "z = tf.subtract(x, y)\n",
        "z = x - y\n",
        "\n",
        "z = tf.multiply(x, y) # element wise multiplication\n",
        "z = tf.divide(x, y)\n",
        "\n",
        "z = tf.tensordot(x, y, axes=1) # element wise multiplication then addition\n",
        "z = tf.reduce_sum(x * y, axis=0)\n",
        "\n",
        "z = x ** 3 # element wise exponent\n",
        "\n",
        "x = tf.random.normal((2,3))\n",
        "y = tf.random.normal((3,4))\n",
        "z = tf.matmul(x,y)\n",
        "z = x @ y # same as matmul\n",
        "\n",
        "#print(z)\n",
        "\n",
        "# Indexing\n",
        "x = tf.constant([0,1,2,3,4,4,2,3])\n",
        "\n",
        "\"\"\"\n",
        "print(x[:])\n",
        "print(x[1:])\n",
        "print(x[1:3]) # [start:end] start is inclusive but end is not\n",
        "print(x[::2]) # [start::step]\n",
        "print(x[::-1]) # reverse order\n",
        "\"\"\"\n",
        "\n",
        "indices=tf.constant([0,5]) # just a list\n",
        "x_ind = tf.gather(x, indices) # get elements in the indexes of x\n",
        "\n",
        "#print(x_ind)\n",
        "\n",
        "x = tf.constant([[1,2],[3,4],[5,6],[7,8]]) # (4,2)\n",
        "\"\"\"\n",
        "print(x[0,1]) # seperate dimensions by comma, give me first element and the second element of it\n",
        "print(x[0:2,:])\n",
        "\"\"\"\n",
        "\n",
        "# Reshaping\n",
        "x = tf.range(9)\n",
        "\n",
        "print(x)\n",
        "x = tf.reshape(x, (3,3))\n",
        "\n",
        "print(tf.transpose(x, perm=[1,0])) # make the rows to col\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJaxd6Ql9z-q"
      },
      "source": [
        "1.) Neural Networks with Sequential and Keras API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb9frsEf96OR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871be281-c83f-49bd-8ffc-2f4d8a6d9b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 535,818\n",
            "Trainable params: 535,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1875/1875 - 6s - loss: 0.1876 - accuracy: 0.9434 - 6s/epoch - 3ms/step\n",
            "Epoch 2/5\n",
            "1875/1875 - 4s - loss: 0.0771 - accuracy: 0.9762 - 4s/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "1875/1875 - 5s - loss: 0.0549 - accuracy: 0.9827 - 5s/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "1875/1875 - 4s - loss: 0.0407 - accuracy: 0.9866 - 4s/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "1875/1875 - 4s - loss: 0.0330 - accuracy: 0.9897 - 4s/epoch - 2ms/step\n",
            "313/313 - 1s - loss: 0.0885 - accuracy: 0.9763 - 657ms/epoch - 2ms/step\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# x_train.shape=(60000,28,28) y_train.shape=(60000,)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0 # normalizes data to 0<=x<=1\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# Sequential API (very convenient, not flexible) one input to one output\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(10),\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), # from_logits does a softmax\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy'], # keep track of running accuracy\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)\n",
        "\n",
        "print(type(x_train))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)Char RNN"
      ],
      "metadata": {
        "id": "2JhrrlPEmWIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ASCII text and convert to lowercase\n",
        "filename = “wonderland.txt”\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "# create a mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars — seq_length, 1):\n",
        "seq_in = raw_text[i:i + seq_length]\n",
        "seq_out = raw_text[i + seq_length]\n",
        "dataX.append([char_to_int[char] for char in seq_in])\n",
        "dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(“Total Patterns: “, n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation=’softmax’))\n",
        "model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)\n",
        "model.summary()\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(start,len(pattern))\n",
        "print (“Seed:”)\n",
        "print (‘’.join([int_to_char[value] for value in pattern]))\n",
        "\n",
        "#import sys\n",
        "# generate characters\n",
        "for i in range(500):\n",
        " x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        " x = x / float(n_vocab)\n",
        " prediction = model.predict(x, verbose=0)\n",
        " index = numpy.argmax(prediction)\n",
        " result = int_to_char[index]\n",
        " seq_in = [int_to_char[value] for value in pattern]\n",
        " #sys.stdout.write(result)\n",
        " print(result,end=””)\n",
        " if i%100 == 0 and i != 0:\n",
        " print(“\\n”)\n",
        " pattern.append(index)\n",
        " pattern = pattern[1:len(pattern)]\n",
        "print (“\\nDone.”)"
      ],
      "metadata": {
        "id": "O8LgvFKVmV7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.)Convolutional Neural Network"
      ],
      "metadata": {
        "id": "QTAdJ7qZieWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "    x = layers.Conv2D(32, 3)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "ld2chaK6iiY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.)Regularization with L2 and Dropout"
      ],
      "metadata": {
        "id": "KErY86u-tJrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "    x = layers.Conv2D(32, 3)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "PzL12BpstJhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.) Regularization to prevent overfitting (L2, Dropout, Early stoppage, Data augmentation)"
      ],
      "metadata": {
        "id": "mmHCLAJT6UaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "    x = layers.Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.Dropout(0.5)(x) # drop 0.5 connections between Dense(64) and Dense(10) to help prevent overfitting\n",
        "    outputs = layers.Dense(10)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "85cLkZjq6gf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.) RNNs, GRUs, LSTMs"
      ],
      "metadata": {
        "id": "_eXx9C3YDk9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# image of 28x28 pixels\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0 # normalize data\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# each time step unroll one row of image at a time\n",
        "# timestep 1 takes the first row\n",
        "# timestep 2 takes the second row\n",
        "# In general you wouldn't use sequence models like RNNs instead you would use Convolutional\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(None, 28))) # None because we don't have a specified number of time steps (arbitrary)\n",
        "model.add(layers.SimpleRNN(256, return_sequences=True, activation='tanh')) # output of this is 512 nodes and outputs it\n",
        "model.add(layers.SimpleRNN(256, activation='tanh'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "print(model.summary())\n",
        "print(x_train.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "hGq920VLDk0l",
        "outputId": "8be02dda-c4fe-49d2-a72c-13fa93b125e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b70226cae3b7>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(!ls)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.) Basic model with Fashion Mnist Sample"
      ],
      "metadata": {
        "id": "lk_fpkZSs6P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# images are 28x28 numpy arrays with pixel values between 0 to 255,\n",
        "# labels are arrays of integers of 0 to 9\n",
        "# 0 : t-shirt, 1 : trouser, 2 : pullover, 3 : dress, 4 : coat ... 9 : ankle boot\n",
        "# train_images.shape = (60000, 28, 28)\n",
        "# len(train_labels) = 60000\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
        "               'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# image data is 0-255, so data needs to be preprocessed\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# need to scale the data to 0 - 1 basically normalize the data\n",
        "# the label are fine just the input images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.figure(figsize=(15, 15), dpi=100)\n",
        "for i in range(10):\n",
        "  plt.subplot(5, 5, i + 1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "  plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)), # no parameters only used to format data\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=64, epochs=10) # called fit because it \"fits\" the model to the training data\n",
        "model.evaluate(test_images, test_labels, batch_size=64)\n",
        "# train accuracy is : 0.9090\n",
        "# test accuracy is : 0.8758\n",
        "# the gap is because of overfitting, model does worst on unseen data\n",
        "# an overfitted model memorizes the noise and details in the training data\n",
        "\n",
        "# after model is trained (fit), validated to be good (evaluate), you can use it for\n",
        "# predictions (pred). Note that only in the model.fit does the parameters get changed\n",
        "# model.compile is only used for training\n",
        "\n",
        "# add a softmax (probablity) layer to trained model\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "\n",
        "predictions = probability_model.predict(test_images)\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  true_label, img = true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  true_label = true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "\n",
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()\n",
        "\n",
        "# Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "predictions_single = probability_model.predict(img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9aEzvugKs6Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UFP06FwQpwEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "print(os.listdir())\n",
        "print(os.path.dirname(dataset))\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9h-sJEmJ2__",
        "outputId": "daac0b66-27a1-48a6-96fa-dc7b0f6bbff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 6s 0us/step\n",
            "['.config', 'aclImdb_v1.tar.gz', 'aclImdb', 'sample_data']\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_AW0oxlrHAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}